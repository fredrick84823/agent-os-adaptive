# Code Style Guide

Team coding standards derived from ml-workflow-cloud-functions and dxp-analysis-report projects.

## ğŸ Python Style Standards

### General Formatting
- **Indentation**: 4 spaces (no tabs)
- **Line length**: 120 characters maximum (some projects use 100)
- **Encoding**: UTF-8 with Chinese comments acceptable
- **String formatting**: Use f-strings for string interpolation

### Naming Conventions
```python
# Functions and variables: snake_case
def process_user_data():
    user_count = 0
    
# Classes: PascalCase
class DataProcessor:
    pass
    
# Constants: UPPER_CASE
MAX_RETRY_COUNT = 3
API_ENDPOINT = "https://api.example.com"

# Private methods: leading underscore
def _internal_helper():
    pass
```

### Import Organization
```python
# 1. Standard library imports
import json
import logging
from typing import Dict, List, Optional, Any

# 2. Third-party imports
import pandas as pd
import numpy as np
from flask import Request
import functions_framework

# 3. Local imports
from src.components.core import DataProcessor
from src.utils import tools
from config import settings
```

### Type Hints
```python
# Function signatures with type hints
def process_data(data: Dict[str, Any]) -> Dict[str, Any]:
    """Process input data and return results."""
    pass

# Optional and Union types
from typing import Optional, Union, List

def get_user(user_id: str) -> Optional[Dict[str, str]]:
    """Retrieve user by ID, return None if not found."""
    pass

# Complex type annotations
def batch_process(
    items: List[Dict[str, Any]], 
    batch_size: int = 100
) -> List[Dict[str, Union[str, int]]]:
    """Process items in batches."""
    pass
```

## ğŸ“ Documentation Standards

### Docstring Format
```python
def calculate_metrics(data: pd.DataFrame, metric_type: str) -> Dict[str, float]:
    """
    è¨ˆç®—æŒ‡å®šé¡å‹çš„æŒ‡æ¨™
    
    Args:
        data: åŒ…å«åŸå§‹æ•¸æ“šçš„ DataFrame
        metric_type: æŒ‡æ¨™é¡å‹ ('frequency', 'purchase_power', 'user_profile')
        
    Returns:
        Dict: åŒ…å«è¨ˆç®—çµæœçš„å­—å…¸
        
    Raises:
        ValueError: ç•¶ metric_type ä¸æ”¯æ´æ™‚
        
    Example:
        >>> df = pd.DataFrame({'user_id': [1, 2], 'value': [10, 20]})
        >>> result = calculate_metrics(df, 'frequency')
        >>> print(result['total_users'])
        2
    """
    pass
```

### File Headers
```python
"""
Cloud Function for processing user analytics data

æ­¤æª”æ¡ˆè™•ç†ç”¨æˆ¶åˆ†ææ•¸æ“šï¼ŒåŒ…å«ä»¥ä¸‹åŠŸèƒ½ï¼š
- æ•¸æ“šé è™•ç†å’Œæ¸…æ´—
- é—œéµå­—é »ç‡è¨ˆç®—
- ç”¨æˆ¶è¼ªå»“ç”Ÿæˆ

Author: Team Data Science
Created: 2024-01-01
Last Modified: 2024-07-22
"""
```

## ğŸ”§ Function Design Patterns

### Cloud Function Entry Points
```python
import functions_framework
from flask import Request
import json
import logging

# è¨­å®šæ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@functions_framework.http
def main(request: Request) -> tuple[str, int]:
    """
    HTTP Cloud Function ä¸»è¦é€²å…¥é»
    
    Args:
        request: Flask Request ç‰©ä»¶
        
    Returns:
        tuple: (response_body, status_code)
    """
    try:
        if request.method == 'OPTIONS':
            return handle_cors()
        elif request.method == 'POST':
            return handle_post_request(request)
        elif request.method == 'GET':
            return handle_get_request(request)
        else:
            return json.dumps({
                'error': f'ä¸æ”¯æ´çš„è«‹æ±‚æ–¹æ³•: {request.method}',
                'status': 'error'
            }), 405
            
    except Exception as e:
        logger.error(f"è™•ç†è«‹æ±‚æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}", exc_info=True)
        return json.dumps({
            'error': 'å…§éƒ¨ä¼ºæœå™¨éŒ¯èª¤',
            'status': 'error'
        }), 500
```

### Error Handling Patterns
```python
# Comprehensive error handling
def process_with_retry(data: Dict[str, Any], max_retries: int = 3) -> Dict[str, Any]:
    """è™•ç†æ•¸æ“šä¸¦æ”¯æ´é‡è©¦æ©Ÿåˆ¶"""
    for attempt in range(max_retries):
        try:
            result = expensive_operation(data)
            logger.info(f"è™•ç†æˆåŠŸï¼Œå˜—è©¦æ¬¡æ•¸: {attempt + 1}")
            return result
            
        except TemporaryError as e:
            logger.warning(f"æš«æ™‚æ€§éŒ¯èª¤ (å˜—è©¦ {attempt + 1}/{max_retries}): {e}")
            if attempt == max_retries - 1:
                raise
            time.sleep(2 ** attempt)  # Exponential backoff
            
        except PermanentError as e:
            logger.error(f"æ°¸ä¹…æ€§éŒ¯èª¤: {e}")
            raise
            
    raise RuntimeError("è¶…éæœ€å¤§é‡è©¦æ¬¡æ•¸")
```

### Logging Standards
```python
import logging
from typing import Dict, Any

# çµæ§‹åŒ–æ—¥èªŒè¨˜éŒ„
logger = logging.getLogger(__name__)

def log_operation(operation: str, data: Dict[str, Any], status: str = "success"):
    """è¨˜éŒ„æ“ä½œæ—¥èªŒ"""
    log_entry = {
        "operation": operation,
        "status": status,
        "data_size": len(data) if isinstance(data, (list, dict)) else 1,
        "timestamp": datetime.utcnow().isoformat()
    }
    
    if status == "success":
        logger.info(f"æ“ä½œå®Œæˆ: {operation}", extra=log_entry)
    elif status == "warning":
        logger.warning(f"æ“ä½œè­¦å‘Š: {operation}", extra=log_entry)
    else:
        logger.error(f"æ“ä½œå¤±æ•—: {operation}", extra=log_entry)
```

## ğŸ“Š Data Processing Patterns

### Pandas Operations
```python
import pandas as pd
from typing import List, Dict, Any

def clean_dataframe(df: pd.DataFrame) -> pd.DataFrame:
    """æ¨™æº–åŒ–æ•¸æ“šæ¸…æ´—æµç¨‹"""
    # ç§»é™¤é‡è¤‡å€¼
    df = df.drop_duplicates()
    
    # è™•ç†ç¼ºå¤±å€¼
    df = df.dropna(subset=['required_column'])
    
    # æ•¸æ“šé¡å‹è½‰æ›
    df['date_column'] = pd.to_datetime(df['date_column'])
    df['numeric_column'] = pd.to_numeric(df['numeric_column'], errors='coerce')
    
    # è¨˜éŒ„æ¸…æ´—çµæœ
    logger.info(f"æ•¸æ“šæ¸…æ´—å®Œæˆï¼Œå‰©é¤˜è¨˜éŒ„æ•¸: {len(df)}")
    
    return df

def batch_process_dataframe(
    df: pd.DataFrame, 
    process_func: callable, 
    batch_size: int = 1000
) -> List[Dict[str, Any]]:
    """æ‰¹é‡è™•ç†å¤§å‹ DataFrame"""
    results = []
    total_batches = len(df) // batch_size + (1 if len(df) % batch_size else 0)
    
    for i in range(0, len(df), batch_size):
        batch = df.iloc[i:i + batch_size]
        batch_result = process_func(batch)
        results.extend(batch_result)
        
        logger.info(f"è™•ç†æ‰¹æ¬¡ {i//batch_size + 1}/{total_batches}")
    
    return results
```

### Async Processing Patterns
```python
import asyncio
import aiohttp
from typing import List, Dict, Any

async def process_async_batch(
    items: List[Dict[str, Any]], 
    semaphore_limit: int = 10
) -> List[Dict[str, Any]]:
    """ç•°æ­¥æ‰¹é‡è™•ç†"""
    semaphore = asyncio.Semaphore(semaphore_limit)
    
    async def process_item(item: Dict[str, Any]) -> Dict[str, Any]:
        async with semaphore:
            # è™•ç†å–®å€‹é …ç›®
            result = await expensive_async_operation(item)
            return result
    
    tasks = [process_item(item) for item in items]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    # è™•ç†ç•°å¸¸
    successful_results = []
    for i, result in enumerate(results):
        if isinstance(result, Exception):
            logger.error(f"è™•ç†é …ç›® {i} å¤±æ•—: {result}")
        else:
            successful_results.append(result)
    
    return successful_results
```

## ğŸ§ª Testing Standards

### Test Structure
```python
import pytest
from unittest.mock import Mock, patch
from src.main import process_data

class TestDataProcessing:
    """æ•¸æ“šè™•ç†åŠŸèƒ½æ¸¬è©¦"""
    
    def setup_method(self):
        """æ¯å€‹æ¸¬è©¦æ–¹æ³•å‰çš„è¨­ç½®"""
        self.sample_data = {
            'user_id': '12345',
            'events': [{'type': 'view', 'timestamp': '2024-01-01'}]
        }
    
    def test_process_data_success(self):
        """æ¸¬è©¦æ­£å¸¸æ•¸æ“šè™•ç†"""
        result = process_data(self.sample_data)
        
        assert result['status'] == 'success'
        assert 'processed_data' in result
        assert len(result['processed_data']) > 0
    
    def test_process_data_empty_input(self):
        """æ¸¬è©¦ç©ºè¼¸å…¥è™•ç†"""
        with pytest.raises(ValueError, match="è¼¸å…¥æ•¸æ“šä¸èƒ½ç‚ºç©º"):
            process_data({})
    
    @patch('src.main.external_api_call')
    def test_process_data_with_mock(self, mock_api):
        """æ¸¬è©¦å¤–éƒ¨ API èª¿ç”¨çš„æ¨¡æ“¬"""
        mock_api.return_value = {'status': 'ok', 'data': []}
        
        result = process_data(self.sample_data)
        
        mock_api.assert_called_once()
        assert result['status'] == 'success'
```

### Integration Test Patterns
```python
import pytest
from google.cloud import storage
from src.utils.gcs_utils import upload_to_gcs

@pytest.mark.integration
class TestGCSIntegration:
    """GCS æ•´åˆæ¸¬è©¦"""
    
    @pytest.fixture
    def gcs_client(self):
        """GCS å®¢æˆ¶ç«¯ fixture"""
        return storage.Client()
    
    def test_upload_file_to_gcs(self, gcs_client):
        """æ¸¬è©¦æ–‡ä»¶ä¸Šå‚³åˆ° GCS"""
        test_data = "æ¸¬è©¦æ•¸æ“šå…§å®¹"
        bucket_name = "test-bucket"
        file_path = "test/file.txt"
        
        result = upload_to_gcs(test_data, bucket_name, file_path)
        
        assert result['status'] == 'success'
        
        # é©—è­‰æ–‡ä»¶ç¢ºå¯¦ä¸Šå‚³
        bucket = gcs_client.bucket(bucket_name)
        blob = bucket.blob(file_path)
        assert blob.exists()
        
        # æ¸…ç†æ¸¬è©¦æ•¸æ“š
        blob.delete()
```

## ğŸ”§ Configuration Management

### Environment Configuration
```python
import os
from typing import Optional

class Config:
    """æ‡‰ç”¨ç¨‹å¼é…ç½®ç®¡ç†"""
    
    # GCP è¨­å®š
    PROJECT_ID: str = os.getenv('GCP_PROJECT_ID', 'tagtoo-ml-workflow')
    BUCKET_NAME: str = os.getenv('GCS_BUCKET_NAME', 'default-bucket')
    
    # API è¨­å®š
    API_TIMEOUT: int = int(os.getenv('API_TIMEOUT', '30'))
    MAX_RETRIES: int = int(os.getenv('MAX_RETRIES', '3'))
    
    # æ•¸æ“šè™•ç†è¨­å®š
    BATCH_SIZE: int = int(os.getenv('BATCH_SIZE', '1000'))
    PARALLEL_WORKERS: int = int(os.getenv('PARALLEL_WORKERS', '4'))
    
    @classmethod
    def validate(cls) -> bool:
        """é©—è­‰å¿…è¦çš„é…ç½®æ˜¯å¦å­˜åœ¨"""
        required_vars = ['GCP_PROJECT_ID']
        missing_vars = [var for var in required_vars if not getattr(cls, var)]
        
        if missing_vars:
            raise ValueError(f"ç¼ºå°‘å¿…è¦çš„ç’°å¢ƒè®Šæ•¸: {missing_vars}")
        
        return True
```